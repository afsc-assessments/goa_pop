---
title: "CIE explorations"
author: "MK"
date: "2023-07-05"
output: html_document
---
This is a scratch file where I organize, document and perform "T1" tasks from the CIE review.
May create another file or set of quarto pages for public-facing documentation after this is complete.

```{r}
require(dplyr)
require(reshape2)
require(ggplot2)
require(here)
library(tidyverse)

## Ben's extractor fns
source("https://raw.githubusercontent.com/afsc-assessments/goa_dusky/25cd6a57d58a6b4e58266a8efd0d1f1c61051baf/2022/code/functions.R")

theme_set(afscasess::theme_report())
# global
year = 2023
# data 
years = 1961:2021
surv_years <-  c(1990, 1993, 1996, 1999, 2001 ,2003 ,2005, 2007, 2009 ,2011 ,2013 ,2015, 2017, 2019, 2021)

get_bio <- function(x, name = 'Model'){ 
bind_rows(as.numeric(unlist(strsplit(x[415]," "))[3:(2+length(years))]) %>% 
data.frame(value = ., year = years, variable = 'sbio'), 
as.numeric(unlist(strsplit(x[416]," "))[3:(2+length(years))]) %>%   
data.frame(value = ., year = years, variable = 'totbio'),   
as.numeric(unlist(strsplit(x[417]," "))[3:(2+length(years))]) %>%   
data.frame(value = ., year = years, variable = 'recruitment'),  
as.numeric(unlist(strsplit(x[405]," "))[5:(4+length(surv_years))]) %>%  
data.frame(value = ., year = surv_years, variable = 'obs_trawl_bio'),   
as.numeric(unlist(strsplit(x[404]," "))[5:(4+length(surv_years))]) %>%  
data.frame(value = ., year = surv_years, variable = 'pred_trawl_bio'),  
as.numeric(unlist(strsplit(x[406]," "))[5:(4+length(surv_years))]) %>%  
data.frame(value = ., year = surv_years, variable = 'obs_trawl_bio_se')) %>%    
mutate(model = name)    
}   

```
# Base model
Don't monkey with this, but do copy & paste stuff from here to create new models.
The goals is that literally everything from model manipulation to execution can be done via this/some script.
```{r}
base_21 <- here::here("goa_pop",year, "mgmt", "2020.1-2021")
base_21_rep <- readLines(here::here(base_21, "model_20_1.rep"))
base_21_bio <-  get_bio(x = base_21_rep, name = 'Base (2021)')
```

# M Prior
PH comment: I've looked into the tool to estimate a prior for M (the current prior is based on this), we also have a SSC request to look at how the prior may be constraining the estimate of M. Would be a good low-hanging fruit sensitivity. There's also a dangling SSC comment related to M that needs to be addressed this year.

Tier 1 - can take another look at M tool, think Thorson may have something new out there too.

Taskings:
Maia - dig into mortality tool and show alternatives, do a profile over M to get at whether current prior is constraining (SSC comment)

The 2021 model based the prior mean for M on Hamel (2015) using a max age approach, with a "relatively precise" cv of 10%:
```{r}
mean_m_21 = 0.0614
cv_m_21 = 0.1
sd_m_21 = mean_m_21*cv_m_21
hist(rnorm(10000,mean = mean_m_21, sd = sd_m_21), main = 'Prior on M (2021)')
```
## Generate new prior for M using fishlife

By default we are using the latest version which corresponds to Thorosn et al 2023.
```{r}
#remotes::install_git(url = 'https://github.com/James-Thorson-NOAA/FishLife')
library( FishLife )
params = matrix( c("K","M", "G","ln_MASPS"), ncol=2, byrow=TRUE)

png(here('2023','dev','cie_0_newM','fishlife_dwnld.png'),
height = 6, width = 10, unit = 'in', res = 420)
#par(mfrow = c(2,3))
Plot_taxa( Search_species(Genus="Sebastes",Species="alutus")$match_taxonomy, mfrow = c(2,3) )
dev.off()

predict <- Plot_taxa( Search_species(Genus="Sebastes",Species="alutus")$match_taxonomy )
log_mean_M <- predict[[1]]$Mean_pred[6]
log_var_M <- predict[[1]]$Cov_pred[6,6]
log_sd_M <- sqrt(var_M)
mean_M_fl <- exp(log_mean_M + var_M/2) ## get back the MEAN via bias correction
sd_m_fl = mean_M_fl*sqrt(exp(log_sd_M)^2-1) ## conversion to non log space
cv_m_fl <- sd_m_fl/mean_M_fl ## the model uses a CV

## plot comparison of old and new prior
png(here('2023','dev','cie_0_newM','prior_compare.png'))
plot(density(rnorm(10000,mean = mean_m_21, sd = 0.0614*cv_m_21)),
xlim = c(0,0.2),
lwd = 3,
type = 'l', xlab = 'Natural Mortality', main = 'Priors for M')
lines(density(x=rnorm(10000,mean = mean_M_fl, sd = sd_m_fl)), lwd =3, col = 'blue')
legend('topright', col = c('black','blue'),lwd = 3,
legend = c('2021 Prior (0.0614,0.00614)', 'FishBase Prior (0.939,0.0527)'))
dev.off()

```

## Implement new M prior
```{r}
## make & populate a new folder with the base stuff
new_dir <- here('goa_pop','2021_cie','cie_0_newM',paste0(Sys.Date(),'-newM'))
dir.create(new_dir)
file.copy(from = list.files(base_21,full.names = TRUE), to = new_dir, overwrite = TRUE)

## populate the relevant lines in the CTL
ctl_file <- list.files(new_dir, pattern = "*.ctl", full.names = TRUE)
newctl <- read.delim(ctl_file, sep = '\t', header = FALSE)
newctl$V1[which(newctl$V2 == '# mprior')] <- round(mean_M_fl,4)
newctl$V1[which(newctl$V2 == '# cvmprior')] <- round(cv_m_fl,4)
newctl$V1 <- newctl$V1

## save it without misc formatting 
write.table(newctl, ctl_file, quote = FALSE,row.names = FALSE, col.names = FALSE)

## run the model
setwd(new_dir)
shell('admb Model_20_1.tpl')
shell('Model_20_1')

## plot the comparison SSB trajectory
 
newM_rep <- readLines(paste0(new_dir,"model_20_1.rep"))
newm_Bio <-  get_bio(x = newM_rep, name = 'Fishlife M Prior')

tmp <- rbind(newm_Bio, base_21_bio)

filter(tmp, variable %in% c("recruitment", "sbio", "totbio")) %>%
ggplot(., aes(x = year, y = value, color = model)) +
geom_line() +
facet_wrap(~variable, scales = 'free_y')

tmp_surv <- merge(subset(tmp, variable == 'obs_trawl_bio' & model == 'Base (2021)'),
subset(tmp, variable == 'obs_trawl_bio_se' & model == 'Base (2021)'), by = 'year') %>%
select(year, model =model.x,
 value = value.x, se = value.y) %>% mutate(lci = value-se, uci = value + se)

ggplot(NULL, aes(x = year, y = value, color = model)) +
geom_line(data = subset(tmp, variable == 'pred_trawl_bio')) +
geom_point(data = tmp_surv) +
geom_errorbar(data =tmp_surv, aes(ymin = lci, ymax = uci), width =0)

```

## Likelihood profile on M
Obviously even shifting up the mean & uncertainty on M has a big effect.
Let's do a profiling exercise and see if we can't dig out what's happening.
In this case I'm going to assume the same degree of confidence as before (0.1).
The loop should go through, lock up (negate) the lines in the control file and run a new model.

Then I will lapply to pull out the likelihood for each component for each element of mvec (lines 435-453 in rep).
```{r}
mvec = seq(0.02,0.2,0.02)

for(m in seq_along(mvec)){
new_dir <- here('goa_pop','2021_cie','cie_0_newM',paste0(Sys.Date(),'-Mprofile_M=',mvec[m]))
dir.create(new_dir)
file.copy(from = list.files(base_21, full.names = TRUE), to = new_dir, overwrite = TRUE)

## populate the relevant lines in the CTL
ctl_file <- list.files(new_dir, pattern = "*.ctl", full.names = TRUE)
newctl <- read.delim(ctl_file, sep = '\t', header = FALSE)
newctl$V1[which(newctl$V2 == '# mprior')] <- mvec[m]
newctl$V1[which(newctl$V2 == '# cvmprior')] <- 0.1
newctl$V1[which(newctl$V2 == '# ph_m')] <- -1
newctl$V1 <- newctl$V1
write.table(newctl, ctl_file, quote = FALSE,row.names = FALSE, col.names = FALSE)

## run the new model
setwd(new_dir)
shell('admb Model_20_1.tpl')
shell('Model_20_1')
}

## plot likelihoods by component
## note that Cole has some code here to pull things out


```

 
# Fishery vs Survey WAA
Pete says this is a higher priority. Basically want to know if the WAA 
in the observed data is statistically different between fishery and survey (aka does it make 
sense to use the same matrix).

1) Pete needs to get growth func "up and running" ??
2) Revisit survey data for fishery application.
In 2021\Body condition there's a script to run an allometric model,
check out the residuals



# Add Lcomp Data
For the fishery, as a sensitivity we could add length data in the years we don't have age data. An additional step in this is to develop expansion code for length comps (rather than using marginal lengths like we do now).
Need expansion code from Pete.

# Catch History 
There's some uncertainty in the early years of the fishery. Instead of trying to revisit the full reconstruction. Instead, leverage the fact that the model already has early (`wt_ssqcatch`) and late (`wt_ssqcatch2`) weights for the time series of catches and monkey with various levels of confidence in the early time series as-is.

Right now these are both set to "50". Will do a run with the early time series weight set to 10, 25, and 75.
Show what this is equivalent to in terms of a time series (confidence interval), as well as derived quants.

do a quick weighting sensitivity, as the model is set up to weight the 'early' catch and 'later' catch. it would be a time suck rabbit hole to go back and (1) try to figure out how the catch history was constructed to then (2) look at alternative estimators. From previous authors this was done by looking at Japanese catch of 'red rockfish' which was divvied up between pop, northerns, and duskies.


# Plus Group

# F vs Sigma R
Looks like we can simply remove the prior on the sigma R and/or the constraint on the rec-devs,
perhaps a combo of the two.